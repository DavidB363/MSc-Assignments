{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval Assignment 1\n",
    "# Indexing for Web Search\n",
    "\n",
    "# Reg no: 1900885 & 1908839\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the Natural Language Toolkit\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the URLs to variables\n",
    "url = 'http://www.multimediaeval.org/mediaeval2019/memorability/'\n",
    "url2 = 'http://sites.google.com/view/siirh2020/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "requests.models.Response"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "r = requests.get(url)\n",
    "r2 = requests.get(url2)\n",
    "type(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTML Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract HTML from Response object and print.\n",
    "html = r.text\n",
    "html2 = r2.text\n",
    "#print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import BeautifulSoup from bs4.\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "soup2 = BeautifulSoup(html2, \"html.parser\")\n",
    "\n",
    "type(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the text out of the soup and print it.\n",
    "text = soup.get_text()\n",
    "text2 = soup2.get_text()\n",
    "# print(text)\n",
    "# print(text2)\n",
    "# type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A fairly general file writing routine that creates a .txt file, give\n",
    "# the file name and its contents.\n",
    "\n",
    "folder_path = \"M:/My Documents/M Drive material/Information Retrieval/Assignment/Text files\"\n",
    "def write_file(filename,contents):\n",
    "    file_path = folder_path + \"/\" + filename + \".txt\"\n",
    "    file= open(file_path,\"w\",encoding=\"utf-8\")\n",
    "    file.write(contents) \n",
    "    file.close()\n",
    "    \n",
    "write_file(\"text\",text)\n",
    "write_file(\"text2\",text2)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing: Sentence splitting, tokenisation and normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import routines from nltk that perform tokenisation into words and sentences.\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Split into sentences\n",
    "sentences_tokens = sent_tokenize(text)\n",
    "sentences_tokens2 = sent_tokenize(text2)\n",
    "\n",
    "# print(sentences_tokens)\n",
    "# print(sentences_tokens2)\n",
    "# type(sentences_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of strings to a string object, separated by a space.\n",
    "sentences_tokens_str =' '.join( sentences_tokens )\n",
    "sentences_tokens_str2 =' '.join( sentences_tokens2 )\n",
    "\n",
    "# Write to a file.\n",
    "write_file(\"sentences_tokens_str\",sentences_tokens_str)\n",
    "write_file(\"sentences_tokens_str2\",sentences_tokens_str2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into words.\n",
    "words_tokens = word_tokenize(text)\n",
    "words_tokens2 = word_tokenize(text2)\n",
    "# print(words_tokens)\n",
    "# print(words_tokens2)\n",
    "# type(words_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of strings to a string object, separated by a space.\n",
    "words_tokens_str =' '.join( words_tokens )\n",
    "words_tokens_str2 =' '.join( words_tokens2 )\n",
    "\n",
    "# Write to a file.\n",
    "write_file(\"words_tokens_str\",words_tokens_str)\n",
    "write_file(\"words_tokens_str2\",words_tokens_str2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise the text.\n",
    "# Transform upper case letters to lowercase letters.\n",
    "\n",
    "length= len(words_tokens)\n",
    "y=words_tokens\n",
    "type(y)\n",
    "for i in range(length):\n",
    "    y[i] = words_tokens[i].lower()\n",
    "    \n",
    "text_lower = y\n",
    "\n",
    "#print(text_lower)\n",
    "type(text_lower)\n",
    "\n",
    "length= len(words_tokens2)\n",
    "y=words_tokens2\n",
    "type(y)\n",
    "for i in range(length):\n",
    "    y[i] = words_tokens2[i].lower()\n",
    "    \n",
    "text_lower2 = y\n",
    "\n",
    "#print(text_lower)\n",
    "# print(text_lower2)\n",
    "# type(text_lower)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of strings to a string object, separated by a space.\n",
    "text_lower_str =' '.join( text_lower )\n",
    "text_lower_str2 =' '.join( text_lower2 )\n",
    "\n",
    "# Write to a file\n",
    "write_file(\"text_lower_str\",text_lower_str)\n",
    "write_file(\"text_lower_str2\",text_lower_str2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation from the text.\n",
    "\n",
    "punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~+=`|'''\n",
    "\n",
    "y=text_lower\n",
    "length=len(text_lower)\n",
    "\n",
    "for i in range(length):\n",
    "    no_punct = \"\"\n",
    "    for char in text_lower[i]:\n",
    "        \n",
    "        if char not in punctuations:\n",
    "            no_punct = no_punct + char\n",
    "    y[i]=no_punct\n",
    "\n",
    "# Remove empty elements from the list.\n",
    "\n",
    "z=[]\n",
    "for i in range(length):\n",
    "    if y[i] != '':\n",
    "        z.append(y[i])   \n",
    "    \n",
    "text_remove_punc = z\n",
    "   \n",
    "# print(text_remove_punc)\n",
    "\n",
    "y=text_lower2\n",
    "length=len(text_lower2)\n",
    "\n",
    "for i in range(length):\n",
    "    no_punct2 = \"\"\n",
    "    for char in text_lower2[i]:\n",
    "        \n",
    "        if char not in punctuations:\n",
    "            no_punct2 = no_punct2 + char\n",
    "    y[i]=no_punct2\n",
    "\n",
    "# Remove empty elements from the list.\n",
    "\n",
    "z=[]\n",
    "for i in range(length):\n",
    "    if y[i] != '':\n",
    "        z.append(y[i])   \n",
    "    \n",
    "text_remove_punc2 = z\n",
    "   \n",
    "# print(text_remove_punc2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of strings to a string object, separated by a space.\n",
    "text_remove_punc_str =' '.join( text_remove_punc )\n",
    "text_remove_punc_str2 =' '.join( text_remove_punc2 )\n",
    "\n",
    "# Write to a file.\n",
    "write_file(\"text_remove_punc_str\",text_remove_punc_str)\n",
    "write_file(\"text_remove_punc_str2\",text_remove_punc_str2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the nltk imported stopwords from the text.\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "filtered_words = [word for word in text_remove_punc if word not in stopwords.words('english')]\n",
    "filtered_words2 = [word for word in text_remove_punc2 if word not in stopwords.words('english')]\n",
    "# print(filtered_words)\n",
    "# type(filtered_words)\n",
    "# print(filtered_words2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of strings to a string object, separated by a space\n",
    "filtered_words_str =' '.join( filtered_words )\n",
    "filtered_words_str2 =' '.join( filtered_words2 )\n",
    "\n",
    "# Write to a file\n",
    "write_file(\"filtered_words_str\",filtered_words_str)\n",
    "write_file(\"filtered_words_str2\",filtered_words_str2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform parts of speech tagging.\n",
    "\n",
    "text_pos = nltk.pos_tag(filtered_words)\n",
    "# print(text_pos)\n",
    "\n",
    "text_pos2 = nltk.pos_tag(filtered_words2)\n",
    "# print(text_pos2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of tuples (containing two strings) into a list.\n",
    "\n",
    "length=len(text_pos)\n",
    "z=[]\n",
    "for i in range(length):\n",
    "    z.append(text_pos[i][0])\n",
    "    z.append(text_pos[i][1])\n",
    "    \n",
    "# Convert the list of strings to a string object, separated by a space.\n",
    "text_pos_str =' '.join( z )\n",
    "\n",
    "# Write to a file.\n",
    "write_file(\"text_pos_str\",text_pos_str)    \n",
    "\n",
    "# Convert the list of tuples (containing two strings) into a list.\n",
    "\n",
    "length=len(text_pos2)\n",
    "z=[]\n",
    "for i in range(length):\n",
    "    z.append(text_pos2[i][0])\n",
    "    z.append(text_pos2[i][1])\n",
    "    \n",
    "# Convert the list of strings to a string object, separated by a space.\n",
    "text_pos_str2 =' '.join( z )\n",
    "\n",
    "# Write to a file.\n",
    "write_file(\"text_pos_str2\",text_pos_str2)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming /LEMMATISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Lemmatisation  is the process of grouping together the inflected forms of a word so they can be analysed as a single item,\n",
    "#  identified by the word's lemma, or dictionary form' - Wikipedia.\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from collections import defaultdict\n",
    "\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatizer_list = []\n",
    "for token, tag in text_pos:\n",
    "    lemma = lemmatizer.lemmatize(token, tag_map[tag[0]])\n",
    "    # print(type(lemma))\n",
    "    # print(token, \"=>\", lemma)\n",
    "    lemmatizer_list.append(lemma)\n",
    "\n",
    "# print(lemmatizer_list)\n",
    "\n",
    "lemmatizer2 = WordNetLemmatizer()\n",
    "\n",
    "lemmatizer_list2 = []\n",
    "for token2, tag2 in text_pos2:\n",
    "    lemma2 = lemmatizer2.lemmatize(token2, tag_map[tag2[0]])\n",
    "    # print(type(lemma))\n",
    "    # print(token, \"=>\", lemma)\n",
    "    lemmatizer_list2.append(lemma2)\n",
    "\n",
    "# print(lemmatizer_list)\n",
    "# print(lemmatizer_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of strings to a string object, separated by a space.\n",
    "lemmatizer_list_str =' '.join( lemmatizer_list )\n",
    "lemmatizer_list_str2 =' '.join( lemmatizer_list2 )\n",
    "\n",
    "# Write to a file.\n",
    "write_file(\"lemmatizer_list_str\",lemmatizer_list_str)\n",
    "write_file(\"lemmatizer_list_str2\",lemmatizer_list_str2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculation of tf.idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 =lemmatizer_list\n",
    "doc1_set = list(set(doc1))\n",
    "# print(doc1_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 =lemmatizer_list2\n",
    "doc2_set = list(set(doc2))\n",
    "# print(doc2_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of times each word appears in the document.\n",
    "length = len(doc1_set)\n",
    "doc1_count = []\n",
    "for i in range(length):\n",
    "    # print(doc1_set[i], doc1.count(doc1_set[i]))\n",
    "    doc1_count.append(doc1.count(doc1_set[i]))\n",
    "# print(doc1_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of times each word appears in the document.\n",
    "length = len(doc2_set)\n",
    "doc2_count = []\n",
    "for i in range(length):\n",
    "    # print(doc2_set[i], doc2.count(doc2_set[i]))\n",
    "    doc2_count.append(doc2.count(doc2_set[i]))\n",
    "# print(doc2_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pandas dataframes to store the information.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df1=pd.DataFrame(doc1_set,columns=['word'])\n",
    "\n",
    "# Note: 'tf' stands for term frequency - the number of times a term appears in a document.\n",
    "df1['tf_count']=doc1_count\n",
    "# print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df2=pd.DataFrame(doc2_set,columns=['word'])\n",
    "# Note: 'tf' stands for term frequency - the number of times a term appears in a document.\n",
    "df2['tf_count']=doc2_count\n",
    "# print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the two sets of words.\n",
    "doc_combined =doc1_set+doc2_set\n",
    "# print(doc_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the elements are distinct.\n",
    "doc_combined_set = list(set(doc_combined))\n",
    "# print(doc_combined_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe for the combined sets of words.\n",
    "dfc =pd.DataFrame(doc_combined_set,columns=['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(doc_combined_set)\n",
    "\n",
    "x=[]\n",
    "for i in range(length):\n",
    "    tf_count=0\n",
    "    if dfc['word'][i] in doc1_set:\n",
    "        index1=doc1_set.index(dfc['word'][i])\n",
    "        tf_count = tf_count + df1['tf_count'][index1]\n",
    "    x.append(tf_count)\n",
    "\n",
    "dfc['tf_doc1']=x    \n",
    "# print(dfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(doc_combined_set)\n",
    "\n",
    "x=[]\n",
    "for i in range(length):\n",
    "    tf_count=0\n",
    "    if dfc['word'][i] in doc2_set:\n",
    "        index2=doc2_set.index(dfc['word'][i])\n",
    "        tf_count = tf_count + df2['tf_count'][index2]\n",
    "    x.append(tf_count)\n",
    "\n",
    "dfc['tf_doc2']=x    \n",
    "# print(dfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of times a term appears over all documents.\n",
    "# This value is not used in subsequent calculations!\n",
    "\n",
    "length = len(doc_combined_set)\n",
    "x=[]\n",
    "for i in range(length):\n",
    "    tf_count = 0\n",
    "    if dfc['word'][i] in doc1_set:\n",
    "        index1=doc1_set.index(dfc['word'][i])\n",
    "        #print(index1)\n",
    "        #print(df1['count'][index1])\n",
    "        tf_count = tf_count + df1['tf_count'][index1]\n",
    "    if dfc['word'][i] in doc2_set:\n",
    "        index2=doc2_set.index(dfc['word'][i])\n",
    "        #print(index2)\n",
    "        #print(df2['count'][index2])\n",
    "        tf_count = tf_count + df2['tf_count'][index2]\n",
    "    x.append(tf_count)\n",
    "        \n",
    "dfc['tf_count']=x      \n",
    "# print(dfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate df_t - the number of documents a term t appears. \n",
    "\n",
    "length = len(doc_combined_set)\n",
    "\n",
    "x=[]\n",
    "for i in range(length):\n",
    "    j=0\n",
    "    if dfc['word'][i] in doc1_set:\n",
    "        j = j+1\n",
    "    if dfc['word'][i] in doc2_set:\n",
    "        j = j+1   \n",
    "    x.append(j)\n",
    "\n",
    "dfc['df_t']=x    \n",
    "\n",
    "# print(dfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate idf - this is log(N)/df_t (log to any base you like!).\n",
    "# The default is the naural logarithm.\n",
    "import math\n",
    "\n",
    "length = len(doc_combined_set)\n",
    "\n",
    "N=2 # Total number of documents\n",
    "x=[]\n",
    "for i in range(length):\n",
    "    y=N/(dfc['df_t'][i]) \n",
    "    z = math.log(y)\n",
    "    x.append(z)\n",
    "\n",
    "dfc['idf']=x    \n",
    "# print(dfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1178\n"
     ]
    }
   ],
   "source": [
    "# Finally, calculate tf.idf. \n",
    "\n",
    "length = len(doc_combined_set)\n",
    "print(length)\n",
    "\n",
    "x1=[]\n",
    "x2=[]\n",
    "for i in range(length):\n",
    "    y1 =(dfc['tf_doc1'][i])*(dfc['idf'][i])\n",
    "    x1.append(y1)\n",
    "    y2 =(dfc['tf_doc2'][i])*(dfc['idf'][i])\n",
    "    x2.append(y2)\n",
    "\n",
    "dfc['tf_idf_doc1']=x1  \n",
    "dfc['tf_idf_doc2']=x2  \n",
    "# print(dfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort in descending order with respect to the tf.idf.\n",
    "\n",
    "sorted_dfc1 = dfc.sort_values(by='tf_idf_doc1',ascending=False)\n",
    "sorted_dfc2 = dfc.sort_values(by='tf_idf_doc2',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sorted_dfc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sorted_dfc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the words with greatest tf.idf to a file, for each document\n",
    "\n",
    "# print(sorted_dfc1.index)\n",
    "# type(sorted_dfc1.index)\n",
    "\n",
    "indices1=list(sorted_dfc1.index)\n",
    "indices2=list(sorted_dfc2.index)\n",
    "# print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the 200 most relevant words to the index file. \n",
    "\n",
    "Most_relevant = 200\n",
    "x=[]\n",
    "for i in range(0, Most_relevant):\n",
    "    #print(sorted_dfc1['word'][indices1[i]],sorted_dfc1['tf_idf_doc1'][indices1[i]])\n",
    "    x.append(sorted_dfc1['word'][indices1[i]])\n",
    "    x.append(str(sorted_dfc1['tf_idf_doc1'][indices1[i]]))\n",
    "index_doc1 = x\n",
    "# print(x)\n",
    "\n",
    "# Convert the list of strings to a string object, separated by a space\n",
    "index_doc1_str =' '.join( index_doc1 )\n",
    "\n",
    "# Write to a file\n",
    "write_file(\"index_doc1_str\",index_doc1_str)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "Most_relevant = 200\n",
    "x=[]\n",
    "for i in range(0, Most_relevant):\n",
    "    #print(sorted_dfc2['word'][indices2[i]],sorted_dfc2['tf_idf_doc2'][indices2[i]])\n",
    "    x.append(sorted_dfc2['word'][indices2[i]])\n",
    "    x.append(str(sorted_dfc2['tf_idf_doc2'][indices2[i]]))\n",
    "index_doc2 = x\n",
    "\n",
    "# Convert the list of strings to a string object, separated by a space\n",
    "index_doc2_str =' '.join( index_doc2 )\n",
    "\n",
    "# Write to a file\n",
    "write_file(\"index_doc2_str\",index_doc2_str)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test code below. \n",
    "# This simple example has been included so that you can\n",
    "#  see clearly that the steps shown above are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pig', 'dog', 'cat']\n"
     ]
    }
   ],
   "source": [
    "doc1 =['cat','dog','cat','pig','cat','cat','pig','dog']\n",
    "doc1_set = list(set(doc1))\n",
    "print(doc1_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['baboon', 'dog', 'cat', 'bear']\n"
     ]
    }
   ],
   "source": [
    "doc2 =['cat','dog','cat','bear','cat','cat','bear','baboon']\n",
    "doc2_set = list(set(doc2))\n",
    "print(doc2_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pig 2\n",
      "dog 2\n",
      "cat 4\n",
      "[2, 2, 4]\n"
     ]
    }
   ],
   "source": [
    "length = len(doc1_set)\n",
    "doc1_count = []\n",
    "for i in range(length):\n",
    "    print(doc1_set[i], doc1.count(doc1_set[i]))\n",
    "    doc1_count.append(doc1.count(doc1_set[i]))\n",
    "print(doc1_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baboon 1\n",
      "dog 1\n",
      "cat 4\n",
      "bear 2\n",
      "[1, 1, 4, 2]\n"
     ]
    }
   ],
   "source": [
    "length = len(doc2_set)\n",
    "doc2_count = []\n",
    "for i in range(length):\n",
    "    print(doc2_set[i], doc2.count(doc2_set[i]))\n",
    "    doc2_count.append(doc2.count(doc2_set[i]))\n",
    "print(doc2_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  word  tf_count\n",
      "0  pig         2\n",
      "1  dog         2\n",
      "2  cat         4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1=pd.DataFrame(doc1_set,columns=['word'])\n",
    "\n",
    "df1['tf_count']=doc1_count\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     word  tf_count\n",
      "0  baboon         1\n",
      "1     dog         1\n",
      "2     cat         4\n",
      "3    bear         2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df2=pd.DataFrame(doc2_set,columns=['word'])\n",
    "df2['tf_count']=doc2_count\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pig', 'dog', 'cat', 'baboon', 'dog', 'cat', 'bear']\n"
     ]
    }
   ],
   "source": [
    "doc_combined =doc1_set+doc2_set\n",
    "print(doc_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'pig', 'baboon', 'dog', 'bear']\n"
     ]
    }
   ],
   "source": [
    "doc_combined_set = list(set(doc_combined))\n",
    "print(doc_combined_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc =pd.DataFrame(doc_combined_set,columns=['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     word  tf_doc1\n",
      "0     cat        4\n",
      "1     pig        2\n",
      "2  baboon        0\n",
      "3     dog        2\n",
      "4    bear        0\n"
     ]
    }
   ],
   "source": [
    "length = len(doc_combined_set)\n",
    "\n",
    "x=[]\n",
    "for i in range(length):\n",
    "    tf_count=0\n",
    "    if dfc['word'][i] in doc1_set:\n",
    "        index1=doc1_set.index(dfc['word'][i])\n",
    "        tf_count = tf_count + df1['tf_count'][index1]\n",
    "    x.append(tf_count)\n",
    "\n",
    "dfc['tf_doc1']=x    \n",
    "print(dfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     word  tf_doc1  tf_doc2\n",
      "0     cat        4        4\n",
      "1     pig        2        0\n",
      "2  baboon        0        1\n",
      "3     dog        2        1\n",
      "4    bear        0        2\n"
     ]
    }
   ],
   "source": [
    "length = len(doc_combined_set)\n",
    "\n",
    "x=[]\n",
    "for i in range(length):\n",
    "    tf_count=0\n",
    "    if dfc['word'][i] in doc2_set:\n",
    "        index2=doc2_set.index(dfc['word'][i])\n",
    "        tf_count = tf_count + df2['tf_count'][index2]\n",
    "    x.append(tf_count)\n",
    "\n",
    "dfc['tf_doc2']=x    \n",
    "print(dfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     word  tf_doc1  tf_doc2  tf_count\n",
      "0     cat        4        4         8\n",
      "1     pig        2        0         2\n",
      "2  baboon        0        1         1\n",
      "3     dog        2        1         3\n",
      "4    bear        0        2         2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "length = len(doc_combined_set)\n",
    "x=[]\n",
    "for i in range(length):\n",
    "    tf_count = 0\n",
    "    if dfc['word'][i] in doc1_set:\n",
    "        index1=doc1_set.index(dfc['word'][i])\n",
    "        #print(index1)\n",
    "        #print(df1['count'][index1])\n",
    "        tf_count = tf_count + df1['tf_count'][index1]\n",
    "    if dfc['word'][i] in doc2_set:\n",
    "        index2=doc2_set.index(dfc['word'][i])\n",
    "        #print(index2)\n",
    "        #print(df2['count'][index2])\n",
    "        tf_count = tf_count + df2['tf_count'][index2]\n",
    "    x.append(tf_count)\n",
    "        \n",
    "dfc['tf_count']=x      \n",
    "print(dfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     word  tf_doc1  tf_doc2  tf_count  df_t\n",
      "0     cat        4        4         8     2\n",
      "1     pig        2        0         2     1\n",
      "2  baboon        0        1         1     1\n",
      "3     dog        2        1         3     2\n",
      "4    bear        0        2         2     1\n"
     ]
    }
   ],
   "source": [
    "length = len(doc_combined_set)\n",
    "\n",
    "x=[]\n",
    "for i in range(length):\n",
    "    j=0\n",
    "    if dfc['word'][i] in doc1_set:\n",
    "        j = j+1\n",
    "    if dfc['word'][i] in doc2_set:\n",
    "        j = j+1   \n",
    "    x.append(j)\n",
    "\n",
    "dfc['df_t']=x    \n",
    "print(dfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     word  tf_doc1  tf_doc2  tf_count  df_t       idf\n",
      "0     cat        4        4         8     2  0.000000\n",
      "1     pig        2        0         2     1  0.693147\n",
      "2  baboon        0        1         1     1  0.693147\n",
      "3     dog        2        1         3     2  0.000000\n",
      "4    bear        0        2         2     1  0.693147\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "length = len(doc_combined_set)\n",
    "\n",
    "N=2 # Total number of documents\n",
    "x=[]\n",
    "for i in range(length):\n",
    "    y=N/(dfc['df_t'][i]) \n",
    "    z = math.log(y)\n",
    "    x.append(z)\n",
    "\n",
    "dfc['idf']=x    \n",
    "print(dfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     word  tf_doc1  tf_doc2  tf_count  df_t       idf  tf_idf_doc1  \\\n",
      "0     cat        4        4         8     2  0.000000     0.000000   \n",
      "1     pig        2        0         2     1  0.693147     1.386294   \n",
      "2  baboon        0        1         1     1  0.693147     0.000000   \n",
      "3     dog        2        1         3     2  0.000000     0.000000   \n",
      "4    bear        0        2         2     1  0.693147     0.000000   \n",
      "\n",
      "   tf_idf_doc2  \n",
      "0     0.000000  \n",
      "1     0.000000  \n",
      "2     0.693147  \n",
      "3     0.000000  \n",
      "4     1.386294  \n"
     ]
    }
   ],
   "source": [
    "length = len(doc_combined_set)\n",
    "\n",
    "x1=[]\n",
    "x2=[]\n",
    "for i in range(length):\n",
    "    y1 =(dfc['tf_doc1'][i])*(dfc['idf'][i])\n",
    "    x1.append(y1)\n",
    "    y2 =(dfc['tf_doc2'][i])*(dfc['idf'][i])\n",
    "    x2.append(y2)\n",
    "\n",
    "dfc['tf_idf_doc1']=x1  \n",
    "dfc['tf_idf_doc2']=x2  \n",
    "print(dfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dfc1 = dfc.sort_values(by='tf_idf_doc1',ascending=False)\n",
    "sorted_dfc2 = dfc.sort_values(by='tf_idf_doc2',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     word  tf_doc1  tf_doc2  tf_count  df_t       idf  tf_idf_doc1  \\\n",
      "1     pig        2        0         2     1  0.693147     1.386294   \n",
      "0     cat        4        4         8     2  0.000000     0.000000   \n",
      "2  baboon        0        1         1     1  0.693147     0.000000   \n",
      "3     dog        2        1         3     2  0.000000     0.000000   \n",
      "4    bear        0        2         2     1  0.693147     0.000000   \n",
      "\n",
      "   tf_idf_doc2  \n",
      "1     0.000000  \n",
      "0     0.000000  \n",
      "2     0.693147  \n",
      "3     0.000000  \n",
      "4     1.386294  \n"
     ]
    }
   ],
   "source": [
    "print(sorted_dfc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     word  tf_doc1  tf_doc2  tf_count  df_t       idf  tf_idf_doc1  \\\n",
      "4    bear        0        2         2     1  0.693147     0.000000   \n",
      "2  baboon        0        1         1     1  0.693147     0.000000   \n",
      "0     cat        4        4         8     2  0.000000     0.000000   \n",
      "1     pig        2        0         2     1  0.693147     1.386294   \n",
      "3     dog        2        1         3     2  0.000000     0.000000   \n",
      "\n",
      "   tf_idf_doc2  \n",
      "4     1.386294  \n",
      "2     0.693147  \n",
      "0     0.000000  \n",
      "1     0.000000  \n",
      "3     0.000000  \n"
     ]
    }
   ],
   "source": [
    "print(sorted_dfc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([1, 0, 2, 3, 4], dtype='int64')\n",
      "[1, 0, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# Write the words with greatest tf.idf to a file, for each document.\n",
    "\n",
    "print(sorted_dfc1.index)\n",
    "type(sorted_dfc1.index)\n",
    "\n",
    "indices1=list(sorted_dfc1.index)\n",
    "indices2=list(sorted_dfc2.index)\n",
    "print(indices1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pig', '1.3862943611198906', 'cat', '0.0', 'baboon', '0.0']\n"
     ]
    }
   ],
   "source": [
    "Most_relevant = 3\n",
    "x=[]\n",
    "for i in range(0, Most_relevant):\n",
    "    #print(sorted_dfc1['word'][indices1[i]],sorted_dfc1['tf_idf_doc1'][indices1[i]])\n",
    "    x.append(sorted_dfc1['word'][indices1[i]])\n",
    "    x.append(str(sorted_dfc1['tf_idf_doc1'][indices1[i]]))\n",
    "index_doc1 = x\n",
    "print(x)\n",
    "\n",
    "# Convert the list of strings to a string object, separated by a space.\n",
    "index_doc1_toy_str =' '.join( index_doc1 )\n",
    "\n",
    "# Write to a file.\n",
    "write_file(\"index_doc1_toy_str\",index_doc1_toy_str)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "Most_relevant = 3\n",
    "x=[]\n",
    "for i in range(0, Most_relevant):\n",
    "    # print(sorted_dfc2['word'][indices2[i]],sorted_dfc2['tf_idf_doc2'][indices2[i]])\n",
    "    x.append(sorted_dfc2['word'][indices2[i]])\n",
    "    x.append(str(sorted_dfc2['tf_idf_doc2'][indices2[i]]))\n",
    "index_doc2 = x\n",
    "\n",
    "# Convert the list of strings to a string object, separated by a space.\n",
    "index_doc2_toy_str =' '.join( index_doc2 )\n",
    "\n",
    "# Write to a file.\n",
    "write_file(\"index_doc2_toy_str\",index_doc2_toy_str)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'str'>\n",
      "1.3862943611198906\n"
     ]
    }
   ],
   "source": [
    "print(type(sorted_dfc2['word'][indices2[0]]))\n",
    "print(type(sorted_dfc2['tf_idf_doc2'][indices2[0]]))\n",
    "\n",
    "sss=str(sorted_dfc2['tf_idf_doc2'][indices2[0]])\n",
    "print(type(sss))\n",
    "print(sss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
